# core/services/perception.py (Vers√£o com Integra√ß√£o Multimodal)
"""
PerceptionAPI ‚Äì V4.3 (Com Suporte Arquitetural a Multimodal)
"""
import numpy as np
import time
import logging
import uuid # Adicionado para futuros metadados
from typing import Any, Dict, Optional, Tuple

from .security import Security

# ... (imports existentes) ...
# Simular importa√ß√£o dos novos Bridges
try:
    # Assumindo nova estrutura core/services/multimodal/
    from .multimodal.audio_bridge import AudioBridge 
    from .multimodal.vision_bridge import VisionBridge
except ImportError:
    # Fallback Mocks (mantido para execu√ß√£o local)
    class MockAudioBridge:
        def transcribe(self, data, fmt): return "Simula√ß√£o de transcri√ß√£o de √°udio.", 0.9
    class MockVisionBridge:
        def process_image(self, data, fmt): return np.zeros(768), ["mock_tag"]
    AudioBridge, VisionBridge = MockAudioBridge, MockVisionBridge

logger = logging.getLogger("PerceptionAPI")
MAX_INPUT_LENGTH = 4096


class PerceptionAPI:
    """
    Servi√ßo de Percep√ß√£o.
    Entrada ‚Üí Sanitiza√ß√£o ‚Üí Hash ‚Üí Metadados
    """

    def __init__(self, security_service: Optional[Any] = None):
        self.sec = security_service or Security()
        self.audio_bridge = AudioBridge() # <-- NOVO
        self.vision_bridge = VisionBridge() # <-- NOVO
        logger.info("PerceptionAPI inicializado (Suporte Multimodal Ativo).")

    # ---------------------------------------------------------------------
    # PERCEP√á√ÉO PRINCIPAL
    # ---------------------------------------------------------------------

    def perceive(
        self,
        input_data: Any,
        source_type: str = "text",
        file_format: Optional[str] = None, # <-- NOVO: Auxilia na valida√ß√£o bin√°ria
        context_meta: Optional[Dict[str, Any]] = None
    ) -> Tuple[str, Dict[str, Any]]:

        is_sanitized = True
        extra_multimodal_data: Dict[str, Any] = {} # Para embeddings, confian√ßa, etc.
        
        # ============================
        # 1. Percep√ß√£o de TEXTO
        # ============================
        if source_type == "text":
            if not isinstance(input_data, str):
                raise TypeError("PerceptionAPI: input_data deve ser str quando source_type='text'")

            raw_text = input_data.strip()
            # ... (Restante da l√≥gica de sanitiza√ß√£o de texto) ...
            original_length = len(raw_text)
            processed_text = self.sec.sanitize_input(raw_text)

            if len(processed_text) > MAX_INPUT_LENGTH:
                processed_text = processed_text[:MAX_INPUT_LENGTH]
                is_sanitized = False 
            
            processed_length = len(processed_text)
            input_hash = self.sec.hash_state(processed_text)

            logger.info(
                f"[Perception] Texto recebido. len={original_length} ‚Üí {processed_length}, hash={input_hash[:12]}..."
            )

        # ============================
        # 2. Percep√ß√£o MULTIMODAL
        # ============================
        elif source_type in ["audio", "image"]:
            
            if not isinstance(input_data, bytes):
                raise TypeError(f"PerceptionAPI: input_data deve ser bytes para source_type='{source_type}'")

            try:
                if source_type == "audio":
                    # üîí Valida√ß√£o Bin√°ria e Sanitiza√ß√£o Real (via Bridge)
                    transcribed_text, confidence = self.audio_bridge.transcribe(input_data, file_format or "wav")
                    
                    # O texto processado √© o resultado do STT
                    processed_text = f"[√ÅUDIO TRASNCRITO]: {transcribed_text}"
                    extra_multimodal_data["stt_confidence"] = float(confidence)
                    extra_multimodal_data["transcribed_text"] = transcribed_text # Original transcription
                    
                elif source_type == "image":
                    # üîí Valida√ß√£o Bin√°ria e Sanitiza√ß√£o Real (via Bridge)
                    embedding_vector, tags = self.vision_bridge.process_image(input_data, file_format or "jpg")
                    
                    # O texto processado √© uma descri√ß√£o/tags para o OL
                    processed_text = f"[IMAGEM DESCRITA]: Tags: {', '.join(tags)}"
                    extra_multimodal_data["image_tags"] = tags
                    # √â vital persistir o vetor no Hippocampus, n√£o apenas o texto:
                    extra_multimodal_data["vision_embedding"] = embedding_vector.tolist() 

                original_length = len(input_data)
                processed_length = len(processed_text)
                
                # O Hash √© feito no texto processado para fins de PRAG/Governan√ßa
                input_hash = self.sec.hash_state(processed_text)

                logger.info(
                    f"[Perception] Multimodal '{source_type}' processado. Tags/Confian√ßa anexadas. hash={input_hash[:12]}..."
                )

            except ValueError as e:
                # Captura erros de formato/integridade do arquivo bin√°rio (Valida√ß√£o)
                is_sanitized = False # Falha na sanitiza√ß√£o/valida√ß√£o bin√°ria
                processed_text = f"[ERROR: Falha na valida√ß√£o bin√°ria de {source_type}]"
                input_hash = self.sec.hash_state(processed_text)
                original_length = len(input_data)
                processed_length = len(processed_text)
                logger.error(f"[Perception] Falha na valida√ß√£o bin√°ria: {e}")


        # ============================
        # 3. Tipo desconhecido
        # ============================
        else:
            raise ValueError(f"Unknown source type: {source_type}")

        # ============================
        # 4. Metadados finais
        # ============================
        meta = {
            "source_type": source_type,
            "timestamp": time.time(),
            "processed_hash": input_hash,
            "sanitized": is_sanitized, 
            "context": context_meta or {},
            "original_length": original_length,
            "processed_length": processed_length,
        }
        
        # Adiciona dados multimodais ao contexto
        meta["context"].update(extra_multimodal_data)

        return processed_text, meta